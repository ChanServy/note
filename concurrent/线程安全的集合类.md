线程安全集合类可以分为三大类：

遗留的线程安全集合如 Hashtable ， Vector

使用 Collections 装饰的线程安全集合，如：

- Collections.synchronizedCollection
- Collections.synchronizedList
- Collections.synchronizedMap
- Collections.synchronizedSet
- Collections.synchronizedNavigableMap
- Collections.synchronizedNavigableSet 
- Collections.synchronizedSortedMap
- Collections.synchronizedSortedSet

java.util.concurrent.*

重点介绍 java.util.concurrent.* 下的线程安全集合类，可以发现它们有规律，里面包含三类关键词：Blocking、CopyOnWrite、Concurrent

- Blocking 大部分实现基于锁，并提供用来阻塞的方法
- CopyOnWrite 之类容器修改开销相对较重
- Concurrent 类型的容器
  - 内部很多操作使用 cas 优化，一般可以提供较高吞吐量
  - 弱一致性
    - 遍历时弱一致性，例如，当利用迭代器遍历时，如果容器发生修改，迭代器仍然可以继续进行遍历，这时内容是旧的
    - 求大小弱一致性，size 操作未必是 100% 准确
    - 读取弱一致性

> 遍历时如果发生了修改，对于非安全容器来讲，使用 fail-fast 机制也就是让遍历立刻失败，抛出
> ConcurrentModificationException，不再继续遍历

## 1. ConcurrentHashMap 注意

ConcurrentHashMap 是一个线程安全的集合，ConcurrentHashMap 中的方法，每个方法都能保证在多线程下是原子的，线程安全的，但是业务中有时候会有需求，用 ConcurrentHashMap 中的方法组合使用从而实现某些业务场景，组合去操作同一个共享资源。这样的话，组合的这段代码就可能有线程安全问题。意思就是每个方法是原子的，但是方法组合使用，这段代码就不能保证原子了，其实这儿在前面也提到过。

比如说有一个需求，需要从 map 集合中检查某个 key 是否存在，如果不存在，说明这个 key 是没有的，没有的话需要 put 一个 key，并且给这个 key 对应的 value 一个初始值，将来要累加。

例如下面的代码：

```java
Integer counter = map.get(word);
int newValue = counter == null ? 1 : counter + 1;
map.put(word, newValue);
```

这就是组合使用了。上面的案例中，其实如果在多线程的情况下，ConcurrentHashMap 的 map 对象是一个共享资源，尽管 ConcurrentHashMap 的 get 和 put 方法每个都是原子的，但是这一段代码组合起来就是线程不安全的。

ConcurrentHashMap 提供解决这种场景的方案：

```java
// 注意不能使用 putIfAbsent，此方法返回的是上一次的 value，首次调用返回 null
LongAdder value = map.computeIfAbsent(word, (key) -> new LongAdder());//LongAdder原子累加器，基础是从0开始实现累加效果的
value.increment();// 0-->1、1-->2
```

上面代码中，如果 map 中这个 key 没有，生成一个累加器，将累加器返回；key 有的话，就无需再创建累加器了，一直是同一个累加器来执行这个累加操作的。

## 2. JDK7 中 HashMap 的并发死链

注意，HashMap 的并发死链只是在 JDK7 中才有可能复现，最根本的原因就是并发扩容时的“头插法”。

首先回顾 HashMap 的数据结构：它的数据结构其实就是哈希表，往细了说在 jdk7 中是数组＋链表的结构；在 jdk8 中，就是数组＋链表＋红黑树的结构。

当一个 key 被放入 HashMap 时，首先会计算这个 key 的 hash 值，然后对整个数组的长度进行取模，得到桶下标，然后放入数组下标值与桶下标对应的位置。这样的查找效率非常高，因为是底层数据结构是数组。但是因为数组的容量有限，因此随着存入数量的增加，不可避免会有取模值相同的情况，也就是所谓的“桶下标冲突”，有可能 key 是值不相同的 key 但是它们的桶下标相同，这时 jdk 会让桶下标相同的 key 在数组的同一索引位置形成一个链表，来解决桶下标冲突。将来查找 key 时，先得到 key 的 hash 值，不冲突的情况下直接返回，一旦发生了桶下标冲突，那么再在链表头用 equals 一个一个比较，来查找这个 key。虽然性能上稍微有所损失，但是可解决桶下标冲突。这便是 JDK7 的 HashMap 的基本结构。

有必要说一下：JDK7 为链表头插法，也就是如果没有桶下标冲突，那么数组每个索引的数据都好比链表的头节点，当有了冲突的时候，这个新的冲突的 key 会放在对应索引对应链表的头节点的位置，也就是数组和链表交界的位置节点。但是 JDK8 中，为链表的尾插法，当有了冲突的时候，这个新的冲突的 key 会放在对应索引对应链表的尾节点的位置。

接下来说一说扩容的问题，JDK8 中除了对插入方式进行了改善，并且对扩容策略也进行了升级。首先，随着元素越来越多，必然导致链的长度越来越大，进而查找性能必然受到影响，因为数组结构是查询快，增删慢（因为需要考虑复制成本），链表是增删快，查询慢，所以链表的长度越长，查找的性能越慢。因此为了解决这个问题，JDK 会在数组元素超过阈值（0.75）时，进行一次扩容。

Hashmap的扩容需要满足两个条件：当前数据存储的数量（即size()）大小必须大于等于阈值；当前加入的数据是否发生了hash冲突。因为上面这两个条件，所以存在下面这些情况

（1）、就是hashmap在存值的时候（默认大小为16，负载因子0.75，阈值12），可能达到最后存满16个值的时候，再存入第17个值才会发生扩容现象，因为前16个值，每个值在底层数组中分别占据一个位置，并没有发生hash碰撞。

（2）、当然也有可能存储更多值（超多16个值，最多可以存26个值）都还没有扩容。原理：前11个值全部hash碰撞，存到数组的同一个位置（这时元素个数小于阈值12，不会扩容），后面所有存入的15个值全部分散到数组剩下的15个位置（这时元素个数大于等于阈值，但是每次存入的元素并没有发生hash碰撞，所以不会扩容），前面11+15=26，所以在存入第27个值的时候才同时满足上面两个条件，这时候才会发生扩容现象。

扩容实际上就是产生一个新的数组，容量是原来数组的2倍，并且扩容会重新计算桶下标，原来的链表中的一个一个元素会迁移到新的数组中。扩容后链表的长度缩短，查找性能提升。但是在 JDK7 中也就是因为在多线程下进行扩容时，可能会造成并发死链的问题，这里在 jdk8 中改用了尾插法得到了避免。另外，JDK8 中对于扩容做了改善。HashMap 数组的初始长度为16，当发生扩容时，数组长度最大扩容到64之后，如果链表的长度还是 >= 8（树化阈值），将进行链表转化为红黑树的操作。

HashSet 是不允许存入重复的数据的，如果存入的数据之前存过了，那么 add 方法会返回 false， HashMap 是不允许存入相同的 key 值，如果存的 key 之前存过了相同的 key，那么这后面存的键值对会覆盖前面存的键值对，比如 key valueNew 会覆盖 key valueOld，就相当于更改。那底层是怎么判断重复的呢？比如新存一个值，那么 HashSet 底层会先求出这个值的 Hash值，然后在数组中查找，是否有同样的 Hash 值的元素，如果没有直接存入，如果有相同的哈希值那么进而使用 equals 判断，看这个哈希值对应的链表上是否有 equals 判断后返回 true 的节点，有的话证明之前存过相同的值，那么 add 方法返回 false，没有的话证明之前没有存过，add 方法返回 true。或者新存一个键值对，那么在 HashMap 中就是判断 key 值，原理和 HashSet 一致。

[程序员小灰——什么是 HashMap](https://mp.weixin.qq.com/s?__biz=MzIxMjE5MTE1Nw==&mid=2653191907&idx=1&sn=876860c5a9a6710ead5dd8de37403ffc&chksm=8c990c39bbee852f71c9dfc587fd70d10b0eab1cca17123c0a68bf1e16d46d71717712b91509&scene=21#wechat_redirect)

## 3. JDK8 的 ConcurrentHashMap

### 3.1 重要属性和内部类

```java
// 默认为 0
// 当初始化时, 为 -1
// 当扩容时, 为 -(1 + 扩容线程数)
// 当初始化或扩容完成后，为 下一次的扩容的阈值大小
private transient volatile int sizeCtl;
// 整个 ConcurrentHashMap 就是一个 Node[]
static class Node<K,V> implements Map.Entry<K,V> {}
// hash 表
transient volatile Node<K,V>[] table;
// 扩容时的 新 hash 表
private transient volatile Node<K,V>[] nextTable;
// 扩容时如果某个 bin 迁移完毕, 用 ForwardingNode 作为旧 table bin 的头结点
static final class ForwardingNode<K,V> extends Node<K,V> {}
// 用在 compute 以及 computeIfAbsent 时, 用来占位, 计算完成后替换为普通 Node
static final class ReservationNode<K,V> extends Node<K,V> {}
// 作为 treebin 的头节点, 存储 root 和 first
static final class TreeBin<K,V> extends Node<K,V> {}
// 作为 treebin 的节点, 存储 parent, left, right
static final class TreeNode<K,V> extends Node<K,V> {}
```

### 3.2 重要方法

```java
// 获取 Node[] 中第 i 个 Node
static final <K,V> Node<K,V> tabAt(Node<K,V>[] tab, int i)
 
// cas 修改 Node[] 中第 i 个 Node 的值, c 为旧值, v 为新值
static final <K,V> boolean casTabAt(Node<K,V>[] tab, int i, Node<K,V> c, Node<K,V> v)
 
// 直接修改 Node[] 中第 i 个 Node 的值, v 为新值
static final <K,V> void setTabAt(Node<K,V>[] tab, int i, Node<K,V> v)
```

### 3.3 构造器分析

可以看到实现了懒惰初始化，在构造方法中仅仅计算了 table 的大小，以后在第一次使用时才会真正创建

```java
// initialCapacity 初始容量也就是初始大小；loadFactor 负载因子。3/4 表示将来扩容的阈值；concurrencyLevel 并发度。
public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) {
    if (!(loadFactor > 0.0f) || initialCapacity < 0 || concurrencyLevel <= 0)
        throw new IllegalArgumentException();
    // 如果初始容量小于并发度
    if (initialCapacity < concurrencyLevel) // Use at least as many bins
        // 初始容量最小要保证并发度这么大，将并发度的值给初始容量
        initialCapacity = concurrencyLevel; // as estimated threads
    long size = (long)(1.0 + (long)initialCapacity / loadFactor);
    // tableSizeFor 仍然是保证计算的大小是 2^n, 即 16,32,64 ... 
    int cap = (size >= (long)MAXIMUM_CAPACITY) ?
        MAXIMUM_CAPACITY : tableSizeFor((int)size);
    this.sizeCtl = cap;
}
```

JDK8 中的 ConcurrentHashMap 是懒惰初始化的，上来并不是将数组先创建出来，而是将来真正用到的时候才会创建。在 7 中是不管用没用，一上来就会创建一个 segment 数组，这样会增加内存的成本，在 8 中改进了。

### 3.4 get 流程

```java
public V get(Object key) {
    Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
    // spread 方法能确保返回结果是正数,因为哈希码可能存在负数的情况
    int h = spread(key.hashCode());// 这个h是进行put或者get时真正用到的哈希码
    // 先判断table，table中如果有元素则往里寻找key，否则直接返回null
    if ((tab = table) != null && (n = tab.length) > 0 &&
        // tabAt方法，找到某一个链表，先定位一个桶下标根据桶下标找到对应的链表
        // 如何找到桶下标？(n-1)&h 数组长-1按位与相当于取模运算，但是按位与效率更高
        // 找到头节点看是否不为空，如果不为空，比较头节点的哈希码是否=刚才的key的哈希码
        (e = tabAt(tab, (n - 1) & h)) != null) {
        if ((eh = e.hash) == h) {
            // 进一步判断这个key是否和我们查找的key是一样的 一样返回value，不一样进一步用equals对key进行判断
            if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                return e.val;
        }
        // hash 为负数表示该 bin 在扩容中或是 treebin, 这时调用 find 方法来查找
        else if (eh < 0)
            return (p = e.find(h, key)) != null ? p.val : null;
        // 正常遍历链表, 用 equals 比较
        while ((e = e.next) != null) {
            if (e.hash == h &&
                ((ek = e.key) == key || (ek != null && key.equals(ek))))
                return e.val;
        }
    }
    return null;
}
```

### 3.5 put 流程

以下数组简称（table），链表简称（bin）

```java
public V put(K key, V value) {
    return putVal(key, value, false);
}
final V putVal(K key, V value, boolean onlyIfAbsent) {
    if (key == null || value == null) throw new NullPointerException();
    // 其中 spread 方法会综合高位低位, 具有更好的 hash 性
    int hash = spread(key.hashCode());
    int binCount = 0;
    for (Node<K,V>[] tab = table;;) {
        // f 是链表头节点
        // fh 是链表头结点的 hash
        // i 是链表在 table 中的下标
        Node<K,V> f; int n, i, fh;
        // 要创建 table
        if (tab == null || (n = tab.length) == 0)
            // 初始化 table 使用了 cas, 无需 synchronized 创建成功, 进入下一轮循环
            tab = initTable();
        // 要创建链表头节点
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
            // 添加链表头使用了 cas, 无需 synchronized
            if (casTabAt(tab, i, null,
                         new Node<K,V>(hash, key, value, null)))
                break;
        }
        // 帮忙扩容
        else if ((fh = f.hash) == MOVED)
            // 帮忙之后, 进入下一轮循环
            tab = helpTransfer(tab, f);
        else {
            V oldVal = null;
            // 锁住链表头节点
            synchronized (f) {
                // 再次确认链表头节点没有被移动
                if (tabAt(tab, i) == f) {
                    // 链表
                    if (fh >= 0) {
                        binCount = 1;
                        // 遍历链表
                        for (Node<K,V> e = f;; ++binCount) {
                            K ek;
                            // 找到相同的 key
                            if (e.hash == hash &&
                                ((ek = e.key) == key ||
                                 (ek != null && key.equals(ek)))) {
                                oldVal = e.val;
                                // 更新
                                if (!onlyIfAbsent)
                                    e.val = value;
                                break;
                            }
                            Node<K,V> pred = e;
                            // 已经是最后的节点了, 新增 Node, 追加至链表尾
                            if ((e = e.next) == null) {
                                pred.next = new Node<K,V>(hash, key,
                                                          value, null);
                                break;
                            }
                        }
                    }
                    // 红黑树
                    else if (f instanceof TreeBin) {
                        Node<K,V> p;
                        binCount = 2;
                        // putTreeVal 会看 key 是否已经在树中, 是, 则返回对应的 TreeNode
                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                              value)) != null) {
                            oldVal = p.val;
                            if (!onlyIfAbsent)
                                p.val = value;
                        }
                    }
                }
                // 释放链表头节点的锁
            }

            if (binCount != 0) { 
                if (binCount >= TREEIFY_THRESHOLD)
                    // 如果链表长度 >= 树化阈值(8), 进行链表转为红黑树
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    // 增加 size 计数
    addCount(1L, binCount);
    return null;
}
private final Node<K,V>[] initTable() {
    Node<K,V>[] tab; int sc;
    while ((tab = table) == null || tab.length == 0) {
        if ((sc = sizeCtl) < 0)
            Thread.yield();
        // 尝试将 sizeCtl 设置为 -1（表示初始化 table）
        else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {
            // 获得锁, 创建 table, 这时其它线程会在 while() 循环中 yield 直至 table 创建
            try {
                if ((tab = table) == null || tab.length == 0) {
                    int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
                    Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
                    table = tab = nt;
                    sc = n - (n >>> 2);
                }
            } finally {
                sizeCtl = sc;
            }
            break;
        }
    }
    return tab;
}
// check 是之前 binCount 的个数
private final void addCount(long x, int check) {
    CounterCell[] as; long b, s;
    if (
        // 已经有了 counterCells, 向 cell 累加
        (as = counterCells) != null ||
        // 还没有, 向 baseCount 累加
        !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)
    ) {
        CounterCell a; long v; int m;
        boolean uncontended = true;
        if (
            // 还没有 counterCells
            as == null || (m = as.length - 1) < 0 ||
            // 还没有 cell
            (a = as[ThreadLocalRandom.getProbe() & m]) == null ||
            // cell cas 增加计数失败
            !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))
        ) {
            // 创建累加单元数组和cell, 累加重试
            fullAddCount(x, uncontended);
            return;
        }
        if (check <= 1)
            return;
        // 获取元素个数
        s = sumCount();
    }
    if (check >= 0) {
        Node<K,V>[] tab, nt; int n, sc;
        while (s >= (long)(sc = sizeCtl) && (tab = table) != null &&
               (n = tab.length) < MAXIMUM_CAPACITY) {
            int rs = resizeStamp(n);
            if (sc < 0) {
                if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
                    sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||
                    transferIndex <= 0)
                    break;
                // newtable 已经创建了，帮忙扩容
                if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))
                    transfer(tab, nt);
            }
            // 需要扩容，这时 newtable 未创建
            else if (U.compareAndSwapInt(this, SIZECTL, sc,
                                         (rs << RESIZE_STAMP_SHIFT) + 2))
                transfer(tab, null);
            s = sumCount();
        }
    }
}
```

### 3.6 size 计算流程

size 计算实际发生在 put，remove 改变集合元素的操作之中

- 没有竞争发生，向 baseCount 累加计数
- 有竞争发生，新建 counterCells，向其中的一个 cell 累加计数
  - counterCells 初始有两个 cell
  - 如果计数竞争比较激烈，会创建新的 cell 来累加计数

```java
public int size() {
    long n = sumCount();
    return ((n < 0L) ? 0 :
            (n > (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE :
            (int)n);
}
final long sumCount() {
    CounterCell[] as = counterCells; CounterCell a;
    // 将 baseCount 计数与所有 cell 计数累加
    long sum = baseCount;
    if (as != null) {
        for (int i = 0; i < as.length; ++i) {
            if ((a = as[i]) != null)
                sum += a.value;
        }
    }
    return sum;
}
```

### 3.7 总结

Java 8 数组（Node） +（ 链表 Node | 红黑树 TreeNode ） 以下数组简称（table），链表简称（bin）

- 初始化，使用 cas 来保证并发安全，懒惰初始化 table
- 树化，当 table.length < 64 时，先尝试扩容，超过 64 时，并且 bin.length > 8 时，会将链表树化，树化过程会用 synchronized 锁住链表头
- put，如果该 bin 尚未创建，只需要使用 cas 创建 bin；如果已经有了，锁住链表头进行后续 put 操作，元素添加至 bin 的尾部
- get，无锁操作仅需要保证可见性，扩容过程中 get 操作拿到的是 ForwardingNode 它会让 get 操作在新 table 进行搜索
- 扩容，扩容时以 bin 为单位进行，需要对 bin 进行 synchronized，但这时妙的是其它竞争线程也不是无事可做，它们会帮助把其它 bin 进行扩容，扩容时平均只有 1/6 的节点会把复制到新 table 中
- size，元素个数保存在 baseCount 中，并发时的个数变动保存在 CounterCell[] 当中。最后统计数量时累加即可

> HashMap 允许有空 key 空 value，也就是可以为 null；但是 ConcurrentHashMap 不允许，会抛异常。

[程序员小灰——什么是 ConcurrentHashMap （JDK7）](https://mp.weixin.qq.com/s/1yWSfdz0j-PprGkDgOomhQ)

## 4. LinkedBlockingQueue

### 4.1 基本的入队出队

```java
public class LinkedBlockingQueue<E> extends AbstractQueue<E>
    implements BlockingQueue<E>, java.io.Serializable {
    static class Node<E> {
        E item;
        /**
     * 下列三种情况之一
     * - 真正的后继节点
     * - 自己, 发生在出队时
     * - null, 表示是没有后继节点, 是最后了
     */
        Node<E> next;
        Node(E x) { item = x; }
    }
}
```

初始化链表 `last = head = new Node<E>(null);` Dummy 节点用来占位，item 为 null

![](https://cdn.jsdelivr.net/gh/ChanServy/CDN2@master/concurrent/image/image_95.png)

当一个节点入队 `last = last.next = node;`

![](https://cdn.jsdelivr.net/gh/ChanServy/CDN2@master/concurrent/image/image_96.png)

再来一个节点入队 `last = last.next = node;`

![](https://cdn.jsdelivr.net/gh/ChanServy/CDN2@master/concurrent/image/image_97.png)

出队

```java
Node<E> h = head;
Node<E> first = h.next;
h.next = h; // help GC 自己指向自己，不在被引用，方便垃圾回收
head = first;
E x = first.item;
first.item = null;
return x;
```

`h = head`

![](https://cdn.jsdelivr.net/gh/ChanServy/CDN2@master/concurrent/image/image_98.png)

`first = h.next`

![](https://cdn.jsdelivr.net/gh/ChanServy/CDN2@master/concurrent/image/image_99.png)

`h.next = h`

![](https://cdn.jsdelivr.net/gh/ChanServy/CDN2@master/concurrent/image/image_100.png)

`head = first`

![](https://cdn.jsdelivr.net/gh/ChanServy/CDN2@master/concurrent/image/image_101.png)

```java
// Dummy节点就是占位节点，是空的没有数据的，真正的数据在第一个Node
E x = first.item;
first.item = null;
return x;
```

![](https://cdn.jsdelivr.net/gh/ChanServy/CDN2@master/concurrent/image/image_102.png)

### 4.2 加锁分析

**==高明之处==**在于用了两把锁和 dummy 节点，占位的头节点，是空的，这个和研究 ReentrantLock 底层时阻塞队列的占位 Dummy 节点是一样的，都是空的占位节点，也叫哨兵节点。

- 用一把锁，同一时刻，最多只允许有一个线程（生产者或消费者，二选一）执行
- 用两把锁，同一时刻，可以允许两个线程同时（一个生产者与一个消费者）执行
  - 消费者与消费者线程仍然串行
  - 生产者与生产者线程仍然串行
  - 保证消费者和消费者间的互斥，生产者与生产者间的互斥就行

线程安全分析

- 当节点总数大于 2 时（包括 dummy 节点），putLock 保证的是 last 节点的线程安全，takeLock 保证的是 head 节点的线程安全。两把锁保证了入队和出队没有竞争
- 当节点总数等于 2 时（即一个 dummy 节点，一个正常节点）这时候，仍然是两把锁锁两个对象，不会竞争，由此可见，Dummy 占位节点就是为了有两把锁对象，因为假设没有它的话，剩一个正常节点，是不可以有两把锁的，也没办法提升效率。
- 当节点总数等于 1 时（就一个 dummy 节点）这时 take 线程会被 notEmpty 条件阻塞，有竞争，会阻塞

```java
// 用于 put(阻塞) offer(非阻塞)
private final ReentrantLock putLock = new ReentrantLock();
// 用户 take(阻塞) poll(非阻塞)
private final ReentrantLock takeLock = new ReentrantLock();
```

#### 4.2.1 put 操作

```java
public void put(E e) throws InterruptedException {
    if (e == null) throw new NullPointerException();
    int c = -1;
    Node<E> node = new Node<E>(e);
    final ReentrantLock putLock = this.putLock;
    // count 用来维护元素计数
    final AtomicInteger count = this.count;
    putLock.lockInterruptibly();
    try {
        // 满了等待
        while (count.get() == capacity) {
            // 倒过来读就好: 等待 notFull
            notFull.await();
        }
        // 有空位, 入队且计数加一
        enqueue(node);
        c = count.getAndIncrement(); 
        // 除了自己 put 以外, 队列还有空位, 由自己叫醒其他 put 线程
        if (c + 1 < capacity)
            notFull.signal();
    } finally {
        putLock.unlock();
    }
    // 如果队列中有一个元素, 叫醒 take 线程
    if (c == 0)
        // 这里调用的是 notEmpty.signal() 而不是 notEmpty.signalAll() 是为了减少竞争
        signalNotEmpty();
}
```

#### 4.2.2 take 操作

```java
public E take() throws InterruptedException {
    E x;
    int c = -1;
    final AtomicInteger count = this.count;
    final ReentrantLock takeLock = this.takeLock;
    takeLock.lockInterruptibly();
    try {
        while (count.get() == 0) {
            notEmpty.await();
        }
        x = dequeue();
        c = count.getAndDecrement();
        if (c > 1)
            notEmpty.signal();
    } finally {
        takeLock.unlock();
    }
    // 如果队列中只有一个空位时, 叫醒 put 线程
    // 如果有多个线程进行出队, 第一个线程满足 c == capacity, 但后续线程 c < capacity
    if (c == capacity)
        // 这里调用的是 notFull.signal() 而不是 notFull.signalAll() 是为了减少竞争
        signalNotFull()
        return x;
}
```

> 由 put 唤醒 put 是为了避免信号不足

### 4.3 性能比较

主要列举 LinkedBlockingQueue 与 ArrayBlockingQueue 的性能比较

- Linked 支持有界，Array 强制有界
- Linked 实现是链表，Array 实现是数组
- Linked 是懒惰的，而 Array 需要提前初始化 Node 数组
- Linked 每次入队会生成新 Node，而 Array 的 Node 是提前创建好的
- Linked 两把锁，Array 一把锁

## 5. ConcurrentLinkedQueue

ConcurrentLinkedQueue 的设计与 LinkedBlockingQueue 非常像，也是

- 两把【锁】，同一时刻，可以允许两个线程同时（一个生产者与一个消费者）执行
- dummy 节点的引入让两把【锁】将来锁住的是不同对象，避免竞争
- 只是这【锁】使用了 cas 来实现，LinkedBlockingQueue 的【锁】用的是 ReentrantLock

事实上，ConcurrentLinkedQueue 应用还是非常广泛的

例如之前讲的 Tomcat 的 Connector 结构时，Acceptor 作为生产者向 Poller 消费者传递事件信息时，正是采用了 ConcurrentLinkedQueue 将 SocketChannel 给 Poller 使用。

## 6. CopyOnWriteArrayList

CopyOnWriteArraySet 是它的马甲 底层实现采用了 写入时拷贝 的思想，增删改操作会将底层数组拷贝一份，更改操作在新数组上执行，这时不影响其它线程的并发读，读写分离。 以新增为例：

```java
public boolean add(E e) {
    synchronized (lock) {
        // 获取旧的数组
        Object[] es = getArray();
        int len = es.length;
        // 拷贝新的数组（这里是比较耗时的操作，但不影响其它读线程）
        es = Arrays.copyOf(es, len + 1);
        // 添加新元素
        es[len] = e;
        // 替换旧的数组
        setArray(es);
        return true;
    }
}
```

> 这里的源码版本是 Java 11，在 Java 1.8 中使用的是可重入锁而不是 synchronized

其它读操作并未加锁，例如：

```java
public void forEach(Consumer<? super E> action) {
    Objects.requireNonNull(action);
    for (Object x : getArray()) {
        @SuppressWarnings("unchecked") E e = (E) x;
        action.accept(e);
    }
}
```

适合『读多写少』的应用场景，

### 6.1 get 弱一致性

![](https://cdn.jsdelivr.net/gh/ChanServy/CDN2@master/concurrent/image/image_103.png)

| 时间点 | 操作                         |
| ------ | ---------------------------- |
| 1      | Thread-0 getArray()          |
| 2      | Thread-1 getArray()          |
| 3      | Thread-1 setArray(arrayCopy) |
| 4      | Thread-0 array[index]        |

1 是能得到的。。。

> 不容易测试，但问题确实存在

### 6.2 迭代器弱一致性

```java
CopyOnWriteArrayList<Integer> list = new CopyOnWriteArrayList<>();
list.add(1);
list.add(2);
list.add(3);
Iterator<Integer> iter = list.iterator();
new Thread(() -> {
    list.remove(0);
    System.out.println(list);
}).start();
sleep1s();
while (iter.hasNext()) {
    System.out.println(iter.next());
}
```

就是怎么说呢。。读的可能是旧的，读写并发可能得到的是旧数据，适合读多写少，因为读的效率高，写的效率底，因为每次写都要复制一个新的数组来操作，这样成本高，并且写写是互斥的，CopyOnWriteArrayList 相当于用空间换线程安全。

这弱一致性，将读写分离，不光做到了读读并发，而且还做到了读写并发，只有写写互斥，这比之前的读写锁的效率更高。

>  不要觉得弱一致性就不好
>
>  - 数据库的 MVCC 都是弱一致性的表现
>  - 并发高和一致性是矛盾的，需要权衡
